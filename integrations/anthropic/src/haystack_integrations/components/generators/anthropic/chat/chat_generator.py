from datetime import datetime, timezone
from typing import Any, ClassVar, Optional, Union

from haystack import component, default_from_dict, default_to_dict, logging
from haystack.components.generators.utils import _convert_streaming_chunks_to_chat_message
from haystack.dataclasses.chat_message import ChatMessage
from haystack.dataclasses.streaming_chunk import (
    AsyncStreamingCallbackT,
    ComponentInfo,
    StreamingCallbackT,
    StreamingChunk,
    SyncStreamingCallbackT,
    select_streaming_callback,
)
from haystack.tools import (
    ToolsType,
    _check_duplicate_tool_names,
    deserialize_tools_or_toolset_inplace,
    flatten_tools_or_toolsets,
    serialize_tools_or_toolset,
)
from haystack.utils.auth import Secret, deserialize_secrets_inplace
from haystack.utils.callable_serialization import deserialize_callable, serialize_callable

from anthropic import Anthropic, AsyncAnthropic
from anthropic.resources.messages.messages import Message, RawMessageStreamEvent, Stream
from anthropic.types import (
    MessageParam,
    TextBlockParam,
    ToolParam,
)

from .utils import (
    _convert_anthropic_chunk_to_streaming_chunk,
    _convert_chat_completion_to_chat_message,
    _convert_messages_to_anthropic_format,
    _process_reasoning_contents,
)

logger = logging.getLogger(__name__)


@component
class AnthropicChatGenerator:
    """
    Completes chats using Anthropic's large language models (LLMs).

    It uses [ChatMessage](https://docs.haystack.deepset.ai/docs/data-classes#chatmessage)
    format in input and output. Supports multimodal inputs including text and images.

    You can customize how the text is generated by passing parameters to the
    Anthropic API. Use the `**generation_kwargs` argument when you initialize
    the component or when you run it. Any parameter that works with
    `anthropic.Message.create` will work here too.

    For details on Anthropic API parameters, see
    [Anthropic documentation](https://docs.anthropic.com/en/api/messages).

    Usage example:
    ```python
    from haystack_integrations.components.generators.anthropic import (
        AnthropicChatGenerator,
    )
    from haystack.dataclasses import ChatMessage

    generator = AnthropicChatGenerator(
        model="claude-sonnet-4-20250514",
        generation_kwargs={
            "max_tokens": 1000,
            "temperature": 0.7,
        },
    )

    messages = [
        ChatMessage.from_system(
            "You are a helpful, respectful and honest assistant"
        ),
        ChatMessage.from_user("What's Natural Language Processing?"),
    ]
    print(generator.run(messages=messages))
    ```

    Usage example with images:
    ```python
    from haystack.dataclasses import ChatMessage, ImageContent

    image_content = ImageContent.from_file_path("path/to/image.jpg")
    messages = [
        ChatMessage.from_user(
            content_parts=["What's in this image?", image_content]
        )
    ]
    generator = AnthropicChatGenerator()
    result = generator.run(messages)
    ```
    """

    # The parameters that can be passed to the Anthropic API https://docs.anthropic.com/claude/reference/messages_post
    ALLOWED_PARAMS: ClassVar[list[str]] = [
        "system",
        "tools",
        "tool_choice",
        "max_tokens",
        "metadata",
        "stop_sequences",
        "temperature",
        "top_p",
        "top_k",
        "extra_headers",
        "thinking",
    ]

    def __init__(
        self,
        api_key: Secret = Secret.from_env_var("ANTHROPIC_API_KEY"),  # noqa: B008
        model: str = "claude-sonnet-4-20250514",
        streaming_callback: Optional[StreamingCallbackT] = None,
        generation_kwargs: Optional[dict[str, Any]] = None,
        ignore_tools_thinking_messages: bool = True,
        tools: Optional[ToolsType] = None,
        *,
        timeout: Optional[float] = None,
        max_retries: Optional[int] = None,
    ):
        """
        Creates an instance of AnthropicChatGenerator.

        :param api_key: The Anthropic API key
        :param model: The name of the model to use.
        :param streaming_callback: A callback function that is called when a new token is received from the stream.
            The callback function accepts StreamingChunk as an argument.
        :param generation_kwargs: Other parameters to use for the model. These parameters are all sent directly to
            the Anthropic endpoint. See Anthropic [documentation](https://docs.anthropic.com/claude/reference/messages_post)
            for more details.

            Supported generation_kwargs parameters are:
            - `system`: The system message to be passed to the model.
            - `max_tokens`: The maximum number of tokens to generate.
            - `metadata`: A dictionary of metadata to be passed to the model.
            - `stop_sequences`: A list of strings that the model should stop generating at.
            - `temperature`: The temperature to use for sampling.
            - `top_p`: The top_p value to use for nucleus sampling.
            - `top_k`: The top_k value to use for top-k sampling.
            - `extra_headers`: A dictionary of extra headers to be passed to the model (i.e. for beta features).
            - `thinking`: A dictionary of thinking parameters to be passed to the model.
                The `budget_tokens` passed for thinking should be less than `max_tokens`.
                For more details and supported models, see: [Anthropic Extended Thinking](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking)

        :param ignore_tools_thinking_messages: Anthropic's approach to tools (function calling) resolution involves a
            "chain of thought" messages before returning the actual function names and parameters in a message. If
            `ignore_tools_thinking_messages` is `True`, the generator will drop so-called thinking messages when tool
            use is detected. See the Anthropic [tools](https://docs.anthropic.com/en/docs/tool-use#chain-of-thought-tool-use)
            for more details.
        :param tools: A list of Tool and/or Toolset objects, or a single Toolset, that the model can use.
            Each tool should have a unique name.
        :param timeout:
            Timeout for Anthropic client calls. If not set, it defaults to the default set by the Anthropic client.
        :param max_retries:
            Maximum number of retries to attempt for failed requests. If not set, it defaults to the default set by
            the Anthropic client.

        """
        _check_duplicate_tool_names(flatten_tools_or_toolsets(tools))

        self.api_key = api_key
        self.model = model
        self.generation_kwargs = generation_kwargs or {}
        self.streaming_callback = streaming_callback
        self.timeout = timeout
        self.max_retries = max_retries

        client_kwargs: dict[str, Any] = {"api_key": api_key.resolve_value()}
        # We do this since timeout=None is not the same as not setting it in Anthropic
        if timeout is not None:
            client_kwargs["timeout"] = timeout
        # We do this since max_retries must be an int when passing to Anthropic
        if max_retries is not None:
            client_kwargs["max_retries"] = max_retries

        self.client = Anthropic(**client_kwargs)
        self.async_client = AsyncAnthropic(**client_kwargs)

        self.ignore_tools_thinking_messages = ignore_tools_thinking_messages
        self.tools = tools

    def _get_telemetry_data(self) -> dict[str, Any]:
        """
        Data that is sent to Posthog for usage analytics.
        """
        return {"model": self.model}

    def to_dict(self) -> dict[str, Any]:
        """
        Serialize this component to a dictionary.

        :returns:
            The serialized component as a dictionary.
        """
        callback_name = serialize_callable(self.streaming_callback) if self.streaming_callback else None
        return default_to_dict(
            self,
            model=self.model,
            streaming_callback=callback_name,
            generation_kwargs=self.generation_kwargs,
            api_key=self.api_key.to_dict(),
            ignore_tools_thinking_messages=self.ignore_tools_thinking_messages,
            tools=serialize_tools_or_toolset(self.tools),
            timeout=self.timeout,
            max_retries=self.max_retries,
        )

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> "AnthropicChatGenerator":
        """
        Deserialize this component from a dictionary.

        :param data: The dictionary representation of this component.
        :returns:
            The deserialized component instance.
        """
        deserialize_secrets_inplace(data["init_parameters"], keys=["api_key"])
        deserialize_tools_or_toolset_inplace(data["init_parameters"], key="tools")
        init_params = data.get("init_parameters", {})
        serialized_callback_handler = init_params.get("streaming_callback")
        if serialized_callback_handler:
            data["init_parameters"]["streaming_callback"] = deserialize_callable(serialized_callback_handler)

        return default_from_dict(cls, data)

    def _prepare_request_params(
        self,
        messages: list[ChatMessage],
        generation_kwargs: Optional[dict[str, Any]] = None,
        tools: Optional[ToolsType] = None,
    ) -> tuple[list[TextBlockParam], list[MessageParam], dict[str, Any], list[ToolParam]]:
        """
        Prepare the parameters for the Anthropic API request.

        :param messages: A list of ChatMessage instances representing the input messages.
        :param generation_kwargs: Optional arguments to pass to the Anthropic generation endpoint.
        :param tools: A list of Tool and/or Toolset objects, or a single Toolset, that the model can use.
        Each tool should have a unique name.
        :returns: A tuple containing:
            - system_messages: List of system messages in Anthropic format
            - non_system_messages: List of non-system messages in Anthropic format
            - generation_kwargs: Processed generation kwargs
            - anthropic_tools: List of tools in Anthropic format
        """
        # update generation kwargs by merging with the generation kwargs passed to the run method
        generation_kwargs = {**self.generation_kwargs, **(generation_kwargs or {})}
        disallowed_params = set(generation_kwargs) - set(self.ALLOWED_PARAMS)
        if disallowed_params:
            logger.warning(
                "Model parameters {disallowed_params} are not allowed and will be ignored. "
                "Allowed parameters are {allowed_params}.",
                disallowed_params=disallowed_params,
                allowed_params=self.ALLOWED_PARAMS,
            )
        generation_kwargs = {k: v for k, v in generation_kwargs.items() if k in self.ALLOWED_PARAMS}

        system_messages, non_system_messages = _convert_messages_to_anthropic_format(messages)

        # prompt caching

        # tools management
        tools = tools or self.tools
        flattened_tools = flatten_tools_or_toolsets(tools)
        _check_duplicate_tool_names(flattened_tools)

        anthropic_tools: list[ToolParam] = []
        if flattened_tools:
            for tool in flattened_tools:
                anthropic_tools.append(
                    ToolParam(name=tool.name, description=tool.description, input_schema=tool.parameters)
                )

        return system_messages, non_system_messages, generation_kwargs, anthropic_tools

    def _process_response(
        self,
        response: Union[Message, Stream[RawMessageStreamEvent]],
        streaming_callback: Optional[SyncStreamingCallbackT] = None,
    ) -> dict[str, list[ChatMessage]]:
        """
        Process the response from the Anthropic API.

        :param response: The response from the Anthropic API.
        :param streaming_callback: A callback function that is called when a new token is received from the stream.
        :returns: A dictionary containing the processed response as a list of ChatMessage objects.
        """
        # workaround for https://github.com/DataDog/dd-trace-py/issues/12562
        # we cannot use isinstance(Stream)
        if not isinstance(response, Message):
            chunks: list[StreamingChunk] = []
            model: Optional[str] = None
            tool_call_index = -1
            input_tokens = None
            component_info = ComponentInfo.from_component(self)
            for chunk in response:
                if chunk.type in ["message_start", "content_block_start", "content_block_delta", "message_delta"]:
                    # Extract model from message_start chunks
                    if chunk.type == "message_start":
                        model = chunk.message.model
                        if chunk.message.usage.input_tokens is not None:
                            input_tokens = chunk.message.usage.input_tokens

                    if chunk.type == "content_block_start" and chunk.content_block.type == "tool_use":
                        tool_call_index += 1

                    streaming_chunk = _convert_anthropic_chunk_to_streaming_chunk(
                        chunk, component_info, tool_call_index
                    )
                    chunks.append(streaming_chunk)
                    if streaming_callback:
                        streaming_callback(streaming_chunk)

            completion = _convert_streaming_chunks_to_chat_message(chunks)
            reasoning = _process_reasoning_contents(chunks)

            completion.meta.update(
                {"received_at": datetime.now(timezone.utc).isoformat(), "model": model},
            )

            if input_tokens is not None:
                if "usage" not in completion.meta:
                    completion.meta["usage"] = {}
                completion.meta["usage"]["input_tokens"] = input_tokens

            return {
                "replies": [
                    ChatMessage.from_assistant(
                        text=completion.text,
                        tool_calls=completion.tool_calls,
                        meta=completion.meta,
                        name=completion.name,
                        reasoning=reasoning,
                    )
                ]
            }

        else:
            return {
                "replies": [_convert_chat_completion_to_chat_message(response, self.ignore_tools_thinking_messages)]
            }

    async def _process_response_async(
        self,
        response: Any,
        streaming_callback: Optional[AsyncStreamingCallbackT] = None,
    ) -> dict[str, list[ChatMessage]]:
        """
        Process the response from the Anthropic API asynchronously.

        :param response: The response from the Anthropic API.
        :param streaming_callback: A callback function that is called when a new token is received from the stream.

        :returns:
            A dictionary containing the processed response as a list of ChatMessage objects.
        """
        # workaround for https://github.com/DataDog/dd-trace-py/issues/12562
        if not isinstance(response, Message):
            chunks: list[StreamingChunk] = []
            model: Optional[str] = None
            tool_call_index = -1
            input_tokens = None
            component_info = ComponentInfo.from_component(self)
            async for chunk in response:
                if chunk.type in [
                    "message_start",
                    "content_block_start",
                    "content_block_delta",
                    "message_delta",
                ]:
                    # Extract model from message_start chunks
                    if chunk.type == "message_start":
                        model = chunk.message.model
                        if chunk.message.usage.input_tokens is not None:
                            input_tokens = chunk.message.usage.input_tokens

                    if chunk.type == "content_block_start" and chunk.content_block.type == "tool_use":
                        tool_call_index += 1

                    streaming_chunk = _convert_anthropic_chunk_to_streaming_chunk(
                        chunk, component_info, tool_call_index
                    )
                    chunks.append(streaming_chunk)
                    if streaming_callback:
                        await streaming_callback(streaming_chunk)

            completion = _convert_streaming_chunks_to_chat_message(chunks)
            reasoning = _process_reasoning_contents(chunks)

            completion.meta.update(
                {"received_at": datetime.now(timezone.utc).isoformat(), "model": model},
            )

            if input_tokens is not None:
                if "usage" not in completion.meta:
                    completion.meta["usage"] = {}
                completion.meta["usage"]["input_tokens"] = input_tokens

            return {
                "replies": [
                    ChatMessage.from_assistant(
                        text=completion.text,
                        tool_calls=completion.tool_calls,
                        meta=completion.meta,
                        name=completion.name,
                        reasoning=reasoning,
                    )
                ]
            }
        else:
            return {
                "replies": [_convert_chat_completion_to_chat_message(response, self.ignore_tools_thinking_messages)]
            }

    @component.output_types(replies=list[ChatMessage])
    def run(
        self,
        messages: list[ChatMessage],
        streaming_callback: Optional[StreamingCallbackT] = None,
        generation_kwargs: Optional[dict[str, Any]] = None,
        tools: Optional[ToolsType] = None,
    ) -> dict[str, list[ChatMessage]]:
        """
        Invokes the Anthropic API with the given messages and generation kwargs.

        :param messages: A list of ChatMessage instances representing the input messages.
        :param streaming_callback: A callback function that is called when a new token is received from the stream.
        :param generation_kwargs: Optional arguments to pass to the Anthropic generation endpoint.
        :param tools: A list of Tool and/or Toolset objects, or a single Toolset, that the model can use.
        Each tool should have a unique name. If set, it will override the `tools` parameter set during component
        initialization.
        :returns: A dictionary with the following keys:
            - `replies`: The responses from the model
        """
        system_messages, non_system_messages, generation_kwargs, anthropic_tools = self._prepare_request_params(
            messages, generation_kwargs, tools
        )

        streaming_callback = select_streaming_callback(
            init_callback=self.streaming_callback,
            runtime_callback=streaming_callback,
            requires_async=False,
        )

        response = self.client.messages.create(
            model=self.model,
            messages=non_system_messages,
            system=system_messages,
            tools=anthropic_tools,
            stream=streaming_callback is not None,
            max_tokens=generation_kwargs.pop("max_tokens", 1024),
            **generation_kwargs,
        )

        return self._process_response(response=response, streaming_callback=streaming_callback)

    @component.output_types(replies=list[ChatMessage])
    async def run_async(
        self,
        messages: list[ChatMessage],
        streaming_callback: Optional[StreamingCallbackT] = None,
        generation_kwargs: Optional[dict[str, Any]] = None,
        tools: Optional[ToolsType] = None,
    ) -> dict[str, list[ChatMessage]]:
        """
        Async version of the run method. Invokes the Anthropic API with the given messages and generation kwargs.

        :param messages: A list of ChatMessage instances representing the input messages.
        :param streaming_callback: A callback function that is called when a new token is received from the stream.
        :param generation_kwargs: Optional arguments to pass to the Anthropic generation endpoint.
        :param tools: A list of Tool and/or Toolset objects, or a single Toolset, that the model can use.
        Each tool should have a unique name. If set, it will override the `tools` parameter set during component
        initialization.
        :returns: A dictionary with the following keys:
            - `replies`: The responses from the model
        """
        system_messages, non_system_messages, generation_kwargs, anthropic_tools = self._prepare_request_params(
            messages, generation_kwargs, tools
        )

        streaming_callback = select_streaming_callback(
            init_callback=self.streaming_callback,
            runtime_callback=streaming_callback,
            requires_async=True,
        )

        response = await self.async_client.messages.create(
            model=self.model,
            messages=non_system_messages,
            system=system_messages,
            tools=anthropic_tools,
            stream=streaming_callback is not None,
            max_tokens=generation_kwargs.pop("max_tokens", 1024),
            **generation_kwargs,
        )

        return await self._process_response_async(response, streaming_callback)
