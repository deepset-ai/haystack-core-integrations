# SPDX-FileCopyrightText: 2024-present deepset GmbH <info@deepset.ai>
#
# SPDX-License-Identifier: Apache-2.0
from typing import Any, Dict, List, Optional, Union

from haystack import component, default_from_dict, default_to_dict
from haystack.utils.auth import Secret, deserialize_secrets_inplace
from haystack_integrations.utils.nvidia import NvidiaCloudFunctionsClient

from ._schema import GenerationRequest, GenerationResponse, Message
from .models import NvidiaGeneratorModel

SUPPORTED_MODELS: List[NvidiaGeneratorModel] = [
    NvidiaGeneratorModel.NV_LLAMA2_RLHF_70B,
    NvidiaGeneratorModel.STEERLM_LLAMA_70B,
    NvidiaGeneratorModel.NEMOTRON_STEERLM_8B,
    NvidiaGeneratorModel.NEMOTRON_QA_8B,
]


@component
class NvidiaGenerator:
    """
    A component for generating text using generative models provided by
    [NVIDIA AI Foundation Endpoints](https://www.nvidia.com/en-us/ai-data-science/foundation-models/).

    Usage example:
    ```python
    from haystack_integrations.components.generators.nvidia import NvidiaGenerator

    generator = NvidiaGenerator(
        model=NvidiaGeneratorModel.NV_LLAMA2_RLHF_70B,
        model_arguments={
            "temperature": 0.2,
            "top_p": 0.7,
            "max_tokens": 1024,
            "seed": None,
            "bad": None,
            "stop": None,
        },
    )
    generator.warm_up()

    result = generator.run(prompt="What is the answer?")
    print(result["replies"])
    print(result["meta"])
    ```
    """

    def __init__(
        self,
        model: Union[str, NvidiaGeneratorModel],
        api_key: Secret = Secret.from_env_var("NVIDIA_API_KEY"),
        model_arguments: Optional[Dict[str, Any]] = None,
    ):
        """
        Create a NvidiaGenerator component.

        :param model:
            Name of the model to use for text generation.
            See the [Nvidia catalog](https://catalog.ngc.nvidia.com/ai-foundation-models) to know the  supported models.
        :param api_key:
            Nvidia API key to use for authentication.
        :param model_arguments:
            Additional arguments to pass to the model provider. Different models accept different arguments.
            Search your model in the [Nvidia catalog](https://catalog.ngc.nvidia.com/ai-foundation-models)
            to know the supported arguments.

        :raises ValueError: If `model` is not supported.
        """
        if isinstance(model, str):
            model = NvidiaGeneratorModel.from_str(model)

        self._model = model
        self._api_key = api_key
        self._model_arguments = model_arguments or {}
        # This is initialized in warm_up
        self._model_id = None

        self._client = NvidiaCloudFunctionsClient(
            api_key=api_key,
            headers={
                "Content-Type": "application/json",
                "Accept": "application/json",
            },
        )

        if self._model not in SUPPORTED_MODELS:
            models = ", ".join(e.value for e in NvidiaGeneratorModel)
            msg = f"Model {self._model} is not supported, available models are: {models}"
            raise ValueError(msg)

    def warm_up(self):
        """
        Initializes the component.
        """
        if self._model_id is not None:
            return
        self._model_id = self._client.get_model_nvcf_id(str(self._model))

    def to_dict(self) -> Dict[str, Any]:
        """
        Serializes the component to a dictionary.

        :returns:
            Dictionary with serialized data.
        """
        return default_to_dict(
            self, model=str(self._model), api_key=self._api_key.to_dict(), model_arguments=self._model_arguments
        )

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "NvidiaGenerator":
        """
        Deserializes the component from a dictionary.

        :param data:
            Dictionary to deserialize from.
        :returns:
           Deserialized component.
        """
        init_params = data.get("init_parameters", {})
        deserialize_secrets_inplace(init_params, ["api_key"])
        return default_from_dict(cls, data)

    @component.output_types(replies=List[str], meta=List[Dict[str, Any]])
    def run(self, prompt: str):
        """
        Queries the model with the provided prompt.

        :param prompt:
            Text to be sent to the generative model.
        :returns:
            A dictionary with the following keys:
            - "replies": replies generated by the model.
            - "meta": metadata for each reply.
        """
        if self._model_id is None:
            msg = "The generation model has not been loaded. Call warm_up() before running."
            raise RuntimeError(msg)

        messages = [Message(role="user", content=prompt)]
        request = GenerationRequest(messages=messages, **self._model_arguments).to_dict()
        json_response = self._client.query_function(self._model_id, request)

        replies = []
        meta = []
        data = GenerationResponse.from_dict(json_response)
        for choice in data.choices:
            replies.append(choice.message.content)
            meta.append(
                {
                    "role": choice.message.role,
                    "finish_reason": choice.finish_reason,
                    # The usage field is not part of each choice, so we use reuse it each time
                    "completion_tokens": data.usage.completion_tokens,
                    "prompt_tokens": data.usage.prompt_tokens,
                    "total_tokens": data.usage.total_tokens,
                }
            )

        return {"replies": replies, "meta": meta}
